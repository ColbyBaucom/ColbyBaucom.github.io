---
layout: post
title: Databases and Interactive Data Visualizations
---

In this blog post, we're going to be working with the `sqlite3` module, which allows us to use __databases__ in Python. They sound similar, but databases are different from `pandas` __dataframes__. How exactly are they different? I'm glad you asked!

Let's say we've got a _really_ big dataset. If we were to read all the data into `pandas` dataframe, our machine might have a difficult time shelling out the amount of RAM required to initialize and work on the data. So what do we do? Working on this `pandas` dataframe will be very inefficient, so we need an alternative. The solution to this problem lies in `sqlite3` databases.

The key to working with `sqlite3` databases lies in the fact that with  big datasets like this, we probably don't need to use the _whole_ dataset. We can work with pieces of the dataset at different times. So, we can __query__ the database to obtain the specific information that we want!

Before we explain more about how exactly this process works, let's import some packages, and read in some data as `pandas` dataframes before assembling a database:


```python
# importing some useful packages

import pandas as pd
import numpy as np
import sqlite3
from plotly import express as px
import plotly.graph_objects as go
from plotly.io import write_html
from sklearn.linear_model import LinearRegression
```

## Our Data


```python
interval = "2011-2020"
url = f"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/noaa-ghcn/decades/{interval}.csv"
temperatures = pd.read_csv(url, chunksize = 100000)
temperatures.__next__()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>Year</th>
      <th>VALUE1</th>
      <th>VALUE2</th>
      <th>VALUE3</th>
      <th>VALUE4</th>
      <th>VALUE5</th>
      <th>VALUE6</th>
      <th>VALUE7</th>
      <th>VALUE8</th>
      <th>VALUE9</th>
      <th>VALUE10</th>
      <th>VALUE11</th>
      <th>VALUE12</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ACW00011604</td>
      <td>2011</td>
      <td>-83.0</td>
      <td>-132.0</td>
      <td>278.0</td>
      <td>1040.0</td>
      <td>1213.0</td>
      <td>1663.0</td>
      <td>1875.0</td>
      <td>1723.0</td>
      <td>1466.0</td>
      <td>987.0</td>
      <td>721.0</td>
      <td>428.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ACW00011604</td>
      <td>2012</td>
      <td>121.0</td>
      <td>-98.0</td>
      <td>592.0</td>
      <td>646.0</td>
      <td>1365.0</td>
      <td>1426.0</td>
      <td>1771.0</td>
      <td>1748.0</td>
      <td>1362.0</td>
      <td>826.0</td>
      <td>620.0</td>
      <td>-234.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ACW00011604</td>
      <td>2013</td>
      <td>-104.0</td>
      <td>-93.0</td>
      <td>-48.0</td>
      <td>595.0</td>
      <td>NaN</td>
      <td>1612.0</td>
      <td>1855.0</td>
      <td>1802.0</td>
      <td>1359.0</td>
      <td>1042.0</td>
      <td>601.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>AE000041196</td>
      <td>2011</td>
      <td>1950.0</td>
      <td>2060.0</td>
      <td>2280.0</td>
      <td>2760.0</td>
      <td>3240.0</td>
      <td>3447.0</td>
      <td>3580.0</td>
      <td>3650.0</td>
      <td>3316.0</td>
      <td>2940.0</td>
      <td>2390.0</td>
      <td>1905.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>AE000041196</td>
      <td>2012</td>
      <td>1837.0</td>
      <td>1987.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>99995</th>
      <td>USC00414976</td>
      <td>2013</td>
      <td>917.0</td>
      <td>1012.0</td>
      <td>1191.0</td>
      <td>1578.0</td>
      <td>2107.0</td>
      <td>2626.0</td>
      <td>2649.0</td>
      <td>2848.0</td>
      <td>2656.0</td>
      <td>1872.0</td>
      <td>1124.0</td>
      <td>697.0</td>
    </tr>
    <tr>
      <th>99996</th>
      <td>USC00414976</td>
      <td>2014</td>
      <td>618.0</td>
      <td>788.0</td>
      <td>1063.0</td>
      <td>1690.0</td>
      <td>2358.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2853.0</td>
      <td>2584.0</td>
      <td>2226.0</td>
      <td>1189.0</td>
      <td>1225.0</td>
    </tr>
    <tr>
      <th>99997</th>
      <td>USC00414976</td>
      <td>2015</td>
      <td>743.0</td>
      <td>657.0</td>
      <td>1227.0</td>
      <td>1851.0</td>
      <td>2171.0</td>
      <td>2690.0</td>
      <td>2833.0</td>
      <td>2768.0</td>
      <td>2545.0</td>
      <td>1974.0</td>
      <td>1407.0</td>
      <td>1174.0</td>
    </tr>
    <tr>
      <th>99998</th>
      <td>USC00414976</td>
      <td>2016</td>
      <td>716.0</td>
      <td>1182.0</td>
      <td>1636.0</td>
      <td>2027.0</td>
      <td>2157.0</td>
      <td>2695.0</td>
      <td>2933.0</td>
      <td>2852.0</td>
      <td>2586.0</td>
      <td>2145.0</td>
      <td>1638.0</td>
      <td>974.0</td>
    </tr>
    <tr>
      <th>99999</th>
      <td>USC00414976</td>
      <td>2017</td>
      <td>1029.0</td>
      <td>1392.0</td>
      <td>1643.0</td>
      <td>1948.0</td>
      <td>2131.0</td>
      <td>2582.0</td>
      <td>2801.0</td>
      <td>2699.0</td>
      <td>2482.0</td>
      <td>1948.0</td>
      <td>1538.0</td>
      <td>879.0</td>
    </tr>
  </tbody>
</table>
<p>100000 rows Ã— 14 columns</p>
</div>



The above dataframe `temperatures` contains temperature information recorded between 2011-2020 at different stations around the world:

 - `ID` is the ID number of the station at which each temperature was recorded.
 - `Year` is the year when the temperature was recorded.
 - `VALUEk` gives the temperature recording in hundredths of a degree Celsius during the `k`-th month of the year. So, `VALUE1` corresponds to January, `VALUE2` to February, `VALUE3` to March, etc.
 
But wait, there's something unfamiliar about the code cell above! What does `chunksize = 100000` mean? We talked a bit before about how we want to avoid reading in a TON of data into a `pandas` dataframe. Adding the `chunksize` argument helps with that. Technically, `temperatures` is not a dataframe, but rather an __iterator__ that can cycle through chunks of our entire dataset in sets of $100,000$ rows. Since `temperatures` is an iterator, calling the `.__next__()` magic method advances the iterator and yields what we see above, a chunk of our entire dataset with $100,000$ rows.

Before we move on to our next relevant dataset, we're going to do a bit of cleaning to our all of our temperature data. 


```python
stations = pd.read_csv("https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/noaa-ghcn/station-metadata.csv")
stations.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>LATITUDE</th>
      <th>LONGITUDE</th>
      <th>STNELEV</th>
      <th>NAME</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ACW00011604</td>
      <td>57.7667</td>
      <td>11.8667</td>
      <td>18.0</td>
      <td>SAVE</td>
    </tr>
    <tr>
      <th>1</th>
      <td>AE000041196</td>
      <td>25.3330</td>
      <td>55.5170</td>
      <td>34.0</td>
      <td>SHARJAH_INTER_AIRP</td>
    </tr>
    <tr>
      <th>2</th>
      <td>AEM00041184</td>
      <td>25.6170</td>
      <td>55.9330</td>
      <td>31.0</td>
      <td>RAS_AL_KHAIMAH_INTE</td>
    </tr>
    <tr>
      <th>3</th>
      <td>AEM00041194</td>
      <td>25.2550</td>
      <td>55.3640</td>
      <td>10.4</td>
      <td>DUBAI_INTL</td>
    </tr>
    <tr>
      <th>4</th>
      <td>AEM00041216</td>
      <td>24.4300</td>
      <td>54.4700</td>
      <td>3.0</td>
      <td>ABU_DHABI_BATEEN_AIR</td>
    </tr>
  </tbody>
</table>
</div>



The above dataframe `stations` tells us a bit about the places in which our previous `temps` data was recorded. Notice that the `ID` column matches between the two. `LONGITUDE` and `LATITUDE` gives us the cooordinates of each station, while `NAME` yields the station's name.


```python
countries = pd.read_csv("https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv")
countries.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>FIPS 10-4</th>
      <th>ISO 3166</th>
      <th>Name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>AF</td>
      <td>AF</td>
      <td>Afghanistan</td>
    </tr>
    <tr>
      <th>1</th>
      <td>AX</td>
      <td>-</td>
      <td>Akrotiri</td>
    </tr>
    <tr>
      <th>2</th>
      <td>AL</td>
      <td>AL</td>
      <td>Albania</td>
    </tr>
    <tr>
      <th>3</th>
      <td>AG</td>
      <td>DZ</td>
      <td>Algeria</td>
    </tr>
    <tr>
      <th>4</th>
      <td>AQ</td>
      <td>AS</td>
      <td>American Samoa</td>
    </tr>
  </tbody>
</table>
</div>



The above dataframe `countries` contains the `FIPS 10-4` and `ISO 3166` codes associated with each country in the world. This will be very useful for us, as it allows us to figure out which country our stations are in. Taking a look at the `ID` column of the `temps` and `stations` dataframes, the __first two letters of the ID correspond to a `FIPS 10-4` code__. So by using the `countries` dataframe, we can learn which country in which our temperature data was recorded.

## Creating a Database

In order to create a database, we'll first have to create a __connection__ to a database. We'll call our database `temps.db`.


```python
conn = sqlite3.connect("temps.db") # for connection
```

Running the above code creates the `temps.db` file in our local directory and establishes a link to the newly-formed database. Now, let's give it some data! Namely, the `temps`, `stations`, and `countries` dataframes. We can do this using the `.to_sql()` method as follows. We'll start with `stations`:


```python
# adding `stations` to `temps.db`
stations.to_sql("stations", conn, if_exists = "replace", index = False)
```


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    <ipython-input-5-2f10e6338efd> in <module>
          1 # adding `stations` to `temps.db`
    ----> 2 stations.to_sql("stations", conn, if_exists = "replace", index = False)
    

    NameError: name 'stations' is not defined


And done! Our temps.db now contains `stations` as a table. Let's add `countries` and `temperatures` as next. Before we add `temperatures` though, we'll have to do a bit of data reshaping in order to appropriately use the dataset. I won't explain in detail what we're doing here, but the main idea is that we're taking all rows named `VALUEk` and using the `.stack()` method to stack them into a single column. i.e., we're creating a `Month` column:


```python
def prepare_df(df):
    """
    A function used to prepare and reshape up the temperature dataframe.
    df: the input dataframe
    return: the prepared dataframe
    """
    df = df.set_index(keys=["ID", "Year"])
    df = df.stack()
    df = df.reset_index()
    df = df.rename(columns = {"level_2"  : "Month" , 0 : "Temp"})
    df["Month"] = df["Month"].str[5:].astype(int)
    df["Temp"]  = df["Temp"] / 100
    return(df)
```


```python
# adding `temperatures` to `temps.db`
for df in temperatures:
    df = prepare_df(df)
    df.to_sql("temperatures", conn, if_exists = "append", index = False)
```


```python
# adding `countries` to `temps.db`
countries.to_sql("countries", conn, if_exists = "replace", index = False)
```

We've finished adding all of our information to our database! Now, we can get to the meat and potatoes of the matter at hand- _actually using the data_.

## Querying the Database

__Querying__ our database, or getting specific data that we want is going to look a little strange at first. This is because `sqlite3` databases use a programming language called `SQL`, as the name implies. `SQL` is a bit strange at first, but we'll see what's going on with an example.

Let's say we want to grab each `Year` in which temperatures hotter than 42 degrees Celsius were recorded. We'll do the following using the `.read_sql_query()` method:


```python
# write a SQL command
command = "SELECT year FROM temperatures WHERE temp > 42"

# read sql query on `temps.db` using conn
pd.read_sql_query(command, conn)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1915</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1917</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1929</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1933</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2018</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1917</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1929</td>
    </tr>
  </tbody>
</table>
</div>



So, we've got 7 years in which temperatures above 42 degrees Celsius were recorded (actually there's 5, since 1915 and 1917 occur twice). We can also write queries that grab information from more than one table in the database- ie., grab data from `temperatures`, `stations`, and `countries` at the same time.

As an example, let's try to grab `temp` readings, and the `Month` and `Year` in which they occured for stations located on the equator. So, stations in in which `LATITUDE` = 0.


```python
# write a SQL command
cmd = \
"""
SELECT T.temp, T.month, T.year
FROM temperatures T
LEFT JOIN stations S ON T.id = S.id
WHERE S.latitude==0
"""

# read sql query on `temps.db` using conn
pd.read_sql_query(cmd, conn).head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Temp</th>
      <th>Month</th>
      <th>Year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>23.55</td>
      <td>8</td>
      <td>1956</td>
    </tr>
    <tr>
      <th>1</th>
      <td>24.25</td>
      <td>9</td>
      <td>1956</td>
    </tr>
    <tr>
      <th>2</th>
      <td>24.45</td>
      <td>10</td>
      <td>1956</td>
    </tr>
    <tr>
      <th>3</th>
      <td>24.75</td>
      <td>11</td>
      <td>1956</td>
    </tr>
    <tr>
      <th>4</th>
      <td>24.35</td>
      <td>12</td>
      <td>1956</td>
    </tr>
  </tbody>
</table>
</div>



The `SQL` command used above is a little more complicated, but in short- the `ID` columns of `temperatures` and `stations` are the same. So, the command uses that column to join the two tables together, and return all specified columns.

Now, let's try something a little more complicated. We'll write a function `query_climate_database` that accepts four arguments:

 - `country`, a string giving the name of a country for which data should be returned.
 - `year_begin` and `year_end`, two integers giving the earliest and latest years for which should be returned.
 - `month`, an integer giving the month of the year for which should be returned.
 
The function will return:
 - The station name.
 - The latitude of the station.
 - The longitude of the station.
 - The name of the country in which the station is located.
 - The year in which the reading was taken.
 - The month in which the reading was taken.
 - The average temperature at the specified station during the specified year and month. 


```python
def query_climate_database(country, year_begin, year_end, month):
    """
    Function that queries the `temps.db` database to obtain a dataframe with specifications listed above.
    
    country: a string giving the name of a country for which data should be returned
    year_begin and year_end: two integers giving the earlies and latest years for which shoud be returned
    month: an integer giving the month of the year for which should be returned
    
    Return a dataframe with the information listed above.
    """
    
    # parameters to be used in SQL query
    params = [country, year_begin, year_end, month]

    
    query = \
    """
    SELECT S.name, S.latitude, S.longitude, C.name, T.year, T.month, T.temp
    FROM temperatures T
    LEFT JOIN stations S ON T.id = S.id
    LEFT JOIN countries C ON SUBSTR(S.id, 1, 2) = C.`fips 10-4`
    WHERE C.name == ? AND T.year >= ?  AND ? >= T.year AND T.month == ?
    """
    
    # query database
    return pd.read_sql_query(query, conn, params = params)
```

Let's try it out! We'll ask our function to give us a bunch of info about temperature readings in Austria from April in 1991 to 1998:


```python
query_climate_database("Austria", 1991, 1998, 4).head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>NAME</th>
      <th>LATITUDE</th>
      <th>LONGITUDE</th>
      <th>Name</th>
      <th>Year</th>
      <th>Month</th>
      <th>Temp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>KREMSMUENSTER</td>
      <td>48.05</td>
      <td>14.1331</td>
      <td>Austria</td>
      <td>1991</td>
      <td>4</td>
      <td>8.26</td>
    </tr>
    <tr>
      <th>1</th>
      <td>KREMSMUENSTER</td>
      <td>48.05</td>
      <td>14.1331</td>
      <td>Austria</td>
      <td>1992</td>
      <td>4</td>
      <td>8.85</td>
    </tr>
    <tr>
      <th>2</th>
      <td>KREMSMUENSTER</td>
      <td>48.05</td>
      <td>14.1331</td>
      <td>Austria</td>
      <td>1993</td>
      <td>4</td>
      <td>10.42</td>
    </tr>
    <tr>
      <th>3</th>
      <td>KREMSMUENSTER</td>
      <td>48.05</td>
      <td>14.1331</td>
      <td>Austria</td>
      <td>1994</td>
      <td>4</td>
      <td>8.77</td>
    </tr>
    <tr>
      <th>4</th>
      <td>KREMSMUENSTER</td>
      <td>48.05</td>
      <td>14.1331</td>
      <td>Austria</td>
      <td>1995</td>
      <td>4</td>
      <td>10.03</td>
    </tr>
  </tbody>
</table>
</div>



Looks good!

## Interactive Visualizations

Using the `pyplot` package, we can incorporate this query function we have just written in creating some interesting data visualizations. We'll write a function `temperature_coefficient_plot` that can plot an interactive visualization of how fast temperatures are rising in a given country:


```python
def coef(data_group):
    """
    Auxiliary function that finds the rate of temperature change for given stations.
    
    data_group: the dataframe containing temperature data
    return: the approximate rate of temperature increase, obtained w/ linear regression
    """
    x = data_group[["Year"]] # 2 brackets because X should be a df
    y = data_group["Temp"]   # 1 bracket because y should be a series
    LR = LinearRegression()
    LR.fit(x, y)
    return LR.coef_[0]
```


```python
def temperature_coefficient_plot(country, year_begin, year_end, month, min_obs, **kwargs):
    """
    Function that creates an interactive visualization of how fast temperature is increasing in a given country.
    
    country: a string giving the name of a country for which data should be returned
    year_begin and year_end: two integers giving the earlies and latest years for which shoud be returned
    month: an integer giving the month of the year for which should be returned
    min_obs: the minimum number of years that can be observed in order for a station's data to appear on the plot
    """
     
    # query the appropriate database
    df = query_climate_database(country, year_begin, year_end, month)
    
    # find rate at which countries are getting hotter via linear regression, incorporate into column
    coefs = df.groupby(["NAME", "Month"]).apply(coef)
    coefs = coefs.reset_index()
    
    # merge dataframes, rename accordingly
    df = pd.merge(df, coefs, on = ["NAME"])
    df = df.rename(columns = {0 : "Rate"})
    
    # create figure
    fig = px.scatter_mapbox(df,
                            lat = "LATITUDE",
                            lon = "LONGITUDE", 
                            hover_name = "Temp",
                            color = df["Rate"],
                            zoom = 1,
                            height = 300,
                            mapbox_style="carto-positron",
                            **kwargs)
    
    
    
    fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
    fig.show()
    
temperature_coefficient_plot("Russia", 1915, 2020, 4, 5, opacity = 0.2)
```

{% include russia-ex.html %}

We'll write a few more interesting functions to create interactive data visualizations below.

## Temperature Anomaly Choropleth

A __choropleth__ is a type of graph that uses boundaries between certain regions as a guideline, almost like a map. We'll create a choropleth that gives every country a unique color that represents the proportion of its temperature readings that may be considered 'anomalous'. What makes a temperature reading anomalous? We'll say that this occurs when a temperature reading is more than $2$ standard deviations away form the mean, i.e. its $z$-score is > $2$.


```python
from urllib.request import urlopen
import json

countries_gj_url = "https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/countries.geojson"

# initializing geojson file for choropleth use
with urlopen(countries_gj_url) as response:
    countries_gj = json.load(response)
```


```python
def z_score(x):
    """
    Auxiliary function that computes the z-score of an observation in an array. Compatible with groupby.
    x: the input array
    return: the z-score
    """
    m = np.mean(x)
    s = np.std(x)
    return (x - m)/s

def percent_anomalies(x): 
    return (abs(x) > 2).mean()
```


```python
def anomalous_temperature_map(year_begin, year_end, **kwargs):
    """
    Function that creates a choropleth indicating what percentage of temperature readings in a country read as anomalous.
    
    year_begin: the earliest year to be considered
    year_end: the latest year to be considered
    
    """
    # parameters to be used in query
    params = [year_begin, year_end]
    
    # SQL query to grab country names, years, and temps
    command = """
    SELECT C.name, T.year, T.temp
    FROM temperatures T
    LEFT JOIN stations S ON T.id = S.id
    LEFT JOIN countries C ON SUBSTR(S.id, 1, 2) = C.`fips 10-4`
    WHERE T.year >= ?  AND ? >= T.year
    """
    
    # read in data and drop NaN rows
    df = pd.read_sql_query(command, conn, params = params)
    df = df.dropna(axis = 0)
    
    # obtain z-score for all temperature observations
    df["z"] = df.groupby(["Name"])["Temp"].transform(z_score)
    
    # determine the percent of temperature anomalies
    anomalies = df.groupby("Name")[["z"]].apply(percent_anomalies)
    anomalies = anomalies.reset_index()
    anomalies["z"] *= 100
    
    # create plot
    fig = px.choropleth(anomalies,
                        geojson = countries_gj,
                        locations = "Name",
                        locationmode = "country names",
                        color = "z",
                        **kwargs)
    
    fig.update_layout(margin = {"r":0, "t": 0, "l": 0, "b": 0})
    fig.show()
    
color_map = px.colors.sequential.Inferno
    
anomalous_temperature_map(1950, 2020, 
                          height = 300,
                          color_continuous_scale = color_map)
```

{% include anomalies.html %}

## Increase in Country Temperature Over Time

Lastly, we'll create a function that creates a scatterplot of a country's average temperature every year. Using a linear regression model, we'll calculate and plot a line of best fit on top of our scatterplot in order to visualize a possible relationship between time and temperature.


```python
import plotly.io as pio
import plotly.graph_objects as go

def country_temperature_increase(country, year_begin, year_end, **kwargs):
    """
    Function that creates a scatterplot of time vs. temperature for a given country.
    Also computes linear regression and plots line on top of scatterplot.
    
    country: country to be analzyed
    year_begin: the earliest year to be considered
    year_end: the latest year to be considered
    
    """
    params = [country, year_begin, year_end]
    
    command = \
    """
    SELECT C.name, T.year, T.temp
    FROM temperatures T
    LEFT JOIN stations S ON T.id = S.id
    LEFT JOIN countries C ON SUBSTR(S.id, 1, 2) = C.`fips 10-4`
    WHERE C.name == ? AND T.year >= ? AND ? >= T.year
    """
    
    df = pd.read_sql_query(command, conn, params = params)
    
    
    avg_temp = df.groupby(["Name", "Year"])[["Temp"]].mean()
    avg_temp = avg_temp.reset_index()
    
    pio.templates.default = "plotly_white"
    
    fig1 = px.scatter(data_frame = avg_temp, 
                     x = "Year", 
                     y = "Temp",
                     color = "Temp",
                     size = "Temp",
                     size_max = 8,
                     width = 500,
                     height = 300,
                     opacity = 0.5)
    
    LR = LinearRegression()
    LR.fit(avg_temp[["Year"]], avg_temp["Temp"])
    
    x = np.arange(year_begin, year_end)
    y = LR.coef_[0]*x + LR.intercept_
    regression = pd.DataFrame({"x" : x, "y" : y})
    
    fig2 = px.line(regression, x = "x", y = "y")
    
    fig3 = go.Figure(data=fig1.data + fig2.data)
    # reduce whitespace
    fig3.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
    # show the plot
   
    fig3.show()
    write_html(fig3, "temp_scatterplot_and_regression")
    
country_temperature_increase("Austria", 1915, 2020)
```

{% include temp_scatterplot_and_regression.html %}

Once we're done working with out dataset, we have to make sure to close our database connection to avoid data corruption as well:


```python
conn.close()
```
