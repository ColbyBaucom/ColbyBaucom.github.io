---
layout: post
title: Spectral Clustering
---

In this blog post, we'll go over a tutorial on a simple version of the *spectral clustering* algorithm for clustering data points. 

Before we talk about spectral clustering though, we'll talk a bit about what _clustering_ means in general. First, we'll create some artificial data as follows:


```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```

![unlabeled_blobs](/images/unlabeled_blobs.png)

Using the `make_blobs` function from `sklearn.datasets`, we've made some data in the shape of 2, well... blobs. The process of _clustering_ allows us to label each data point as either being in one cluster or the other. An example of clustering can be seen below, using the `KMeans` method from `sklearn.cluster.Kmeans` to sort each data point into two clusters. We'll use different colors in order to easily demonstrate which points belong to which cluster.


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![labeled_blobs](/images/labeled_blobs.png)

This is great! Looks like the `KMeans` method did a great job of appropriately labeling the data. Now, let's dive a bit deeper. What if we have some more strangely shaped data? We'll use the `make_moons` method to create some... well, moons.


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

![unlabeled_moons](/images/unlabeled_moons.png)

Looks good! It's clear to the observer that our data is shaped into two different clusters- one that looks like a frown and another that looks like a smile. How does our `KMeans` method do here?


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![kmeans_labeled_moons](/images/kmeans_labeled_moons.png)

Hmm... that's not great. It'd be nice if we could find a better method of labeling our data. This is where __spectral clustering__ comes into play, which uses some interesting linear algebra techniques to better assess what clusters our data points lie in.

## Part A - The Similarity Matrix

Our first step towards spectral clustering lies in the construction of the __similarity matrix__. What is a similarity matrix? If we have `n` data points and consider some constant distance between points `epsilon`, then the similarity matrix is an `n` by `n` matrix that satisfies the following properties:

 1. `A[i,j]` = `1` if the `i`th data point is within `epsilon` distance of the `j`th data point
 2. `A[i,j]` = `0` if the `i`th data point is NOT within `epsilon` distance of the `j`th data point
 3. all diagonal entries are `0`
 
Based on these properties, we'll create a function `similarity_matrix`, and use it to construct a similarity matrix `A` as follows:


```python
from sklearn import metrics

# set value of epsilon
epsilon = 0.4

def similarity_matrix(X, epsilon):
    """
    Function that constructs a similarity matrix.
    
    X: matrix containing coordinates of n data points
    epsilon: the max distance between points to be considered 'close together'
    """

    # create matrix of pairwise distances and use it to create similarity matrix
    A = (metrics.pairwise_distances(X) < epsilon*np.ones((n,n))).astype(int)

    # set diagonals to 0
    np.fill_diagonal(A, 0)

    return A

A = similarity_matrix(X, epsilon)
A
```




    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 1, 0],
           ...,
           [0, 0, 0, ..., 0, 1, 1],
           [0, 0, 1, ..., 1, 0, 1],
           [0, 0, 0, ..., 1, 1, 0]])



## Part B - The Binary Norm Cut Objective

In order to proceed, we'll have to get a bit mathematical. 

Let $$C_0$$ and $$C_1$$ be two clusters of the data points. We assume that every data point is in either $$C_0$$ or $$C_1$$. The cluster membership as being specified by `y`. We think of `y[i]` as being the label of point `i`. So, if `y[i] = 1`, then point `i` (and therefore row $$i$$ of $$\mathbf{A}$$) is an element of cluster $$C_1$$.  

The _binary norm cut objective_ of a similarity matrix `A` is defined as:

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

We'll dissect this expression a bit further, but for now the important thing to note is that __the binary norm cut objective measures the goodness of fit of our cluster labels__. So, if $$C_0$$ are $$C_1$$ are very poor at suggesting clusters for the data (like calling `Kmeans` on data created using `make_moons`), then $$N_{\mathbf{A}}(C_0, C_1)$$ will be large. On the other hand, if $$C_0$$ and $$C_1$$ do a good job at predicting appropriate cluster labels, then $$N_{\mathbf{A}}(C_0, C_1)$$ will be notably smaller.

### i.) Cut

Mathematically, we define the *cut* of the clusters $$C_0$$ and $$C_1$$ as:

$$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$

In words, __the *cut* is the number of points that are `epsilon` close, but also in different clusters__. We'll define a function `cut(A,y)` to compute it below:


```python
def cut(A,y):
    """
    Function that returns the cut of two clusters.
    
    A: the similarity matrix
    y: array containing cluster labels for each point
    """
    
    # initialize empty sum
    cut = 0
    
    # iterate across entire matrix
    for i in range(n):
        for j in range(n):
            
            # add 1 to the cut if i and j are close together but in different clusters
            if (y[i] != y[j]):
                cut += A[i,j]
    
    return cut
```

In order to test our `cut` function, we'll call it on the labels `y` created beforehand versus some randomly generated labels. If we're correct about the purpose of the `cut` function, the randomly generated labels should have more overlap in cluster labels between points less than `epsilon` distance apart, i.e. the cut of the randomly generated labels should be _larger_:


```python
true_cut = cut(A, y)
rand_cut = cut(A, np.random.randint(2, size = n))

true_cut, rand_cut
```




    (26.0, 2232.0)



And this checks out! It's much larger!

### ii.) Volume

Mathematically, we define the the *volume* of a cluster $$C_0$$ as:

$$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$

where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the $$i$$th row-sum of $$\mathbf{A}$$, also called the *degree* of $$i$$. Analagous to its geometric definition, __the volume of a cluster measures its size__. We'll define a function `vols(A,y)` to compute it below:


```python
def vols(A, y):
    """
    Function that computes the volume (way to measure the size) of two clusters.
    
    A: the similarity matrix
    y: array containing cluster labels for each point
    return: the volume of each cluster, supplied in a tuple
    """
    
    return (np.sum(A[y == 0]), np.sum(A[y == 1]))
```

Now that we've defined both `cut(A,y)` and `vols(A,y)`, we're ready to create a function that computes the binary norm cut objective of `A`, using the original definition we supplied above:


```python
def normcut(A,y):
    """
    Function that returns the norm cut objective of a similarity matrix,
    which is a way to measure the "goodness of fit" of cluster labels.
    The smaller the output is, the better the cluster labels.
    
    A: the similarity matrix
    y: array containing cluster labels for each point
    """
    
    V = vols(A,y)
    
    return cut(A,y)*(1/V[0] + 1/V[1])
```

Again, let's test it out! We know that the cluster labels contained in `y` should be a much better fit than randomly generated labels. So, if we've defined out function correctly, we'd expect the randomly generated labels to have a _larger_ output. Let's see:


```python
true_norm = normcut(A,y)
rand_norm = rand_cut = normcut(A, np.random.randint(2, size = n))

true_norm, rand_norm
```




    (0.02303682466323045, 1.9929801437511216)



And... this checks out!

## Part C - Computing Labels

So, our goal now should be to minimize the `normcut(A,y)` that we've defined above. However, this might be tricky with the function we've got now- now, it's time for some linear algebra tricks! We're going to define a new vector $$\mathbf{z} \in \mathbb{R}^n$$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$

Using linear algebra, we can obtain the following equation:

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $$\mathbf{D}$$ is the diagonal matrix with nonzero entries $$d_{ii} = d_i$$, and  where $$d_i = \sum_{j = 1}^n a_i$$ is the degree (row-sum) from before.

So, minimizing `normcut(A,y)` is akin to minimizing the matrix definition above. Before we do that, we'll first write a function `transform(A,y)` that can calculate `z`:


```python
def transform(A,y):
    
    # initialize vector z
    z = np.zeros(n)
    
    # calculate volumes
    V = vols(A,y)
    
    # fill z with appropriate values
    z[y == 0] =  1/V[0]
    z[y == 1] = -1/V[1]
    
    return z
```

Now, that we've defined our `transform(A,y)` function, let's check that it actually satisfies the conditions above, i.e. that $$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;$$. Additionally, we'll check another interesting identity, $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$.


```python
# create diagonal matrix D w/ degree entries
D = np.zeros((n, n))
np.fill_diagonal(D, np.sum(A, axis = 0))

# create new vector z
z = transform(A,y)

LHS = normcut(A,y)
RHS = (2*z@(D-A)@z)/(z@D@z)

np.isclose(LHS, RHS)
```




    True




```python
# check identity:
np.isclose(z@D@np.ones(n), 0)
```




    True



## Part D - Minimizing Labels

We're just about ready to minimize the normcut objective. Here's the last small detail (_I promise_) that we should talk about before moving on. Instead of using `z` directly in our calculation, the identity $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$ allows us to use the orthogonal complement of `z`, relative to $$\mathbf{D}\mathbb{1}$$. We'll define two functions `orth` and `orth_obj` that allow us to do this below. 

In the end though, we'll use the `minimize` method form `scipy.optimize` in order to finally minimize the binary norm cut objective.


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
from scipy import optimize

z_ = optimize.minimize(orth_obj, z)
```

## Part E -  Checking our New Labels

Finally, we've got some cluster labels `z_.x` that will help use out with appropriately labeling our moon-shaped data. Let's check to see how well they work below:


```python
z_min = z_.x

# create array of colors

colors = np.zeros(z_min.size)
colors[z_min < -0.0015] = 1

# plot data w/ cluster colors
plt.scatter(X[:,0], X[:,1], c = colors)
```

![orth_labeled_moons](/images/orth_labeled_moons.png)

Awesome! This looks much better than our previous labels generated using `KMeans`, although there's certainly room for improvement. There are a few mislabeled points on the leftmost edge of the yellow moon.

## Part F - Eigenvalues and Eigenvectors

Let's try to find an even better way to label our data points. The previous method of minimizing the orthogonal objective was good, but it could be much better. In this part, we'll take a look at eigenvectors and their use in minimizing the normcut objective. But first: what is an eigenvector? 

Given a matrix $$\mathbf{A}$$, an eigenvector is a vector $$\mathbf{v}$$ such that $$\mathbf{Av} = k\mathbf{v}$$, where $$k$$ is a scalar. In words, __multiplying a matrix by an eigenvector will just result in a scaled version of the eigenvector__. These special types of vectors will aid us in minimizing the following:

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

which the Rayleigh-Ritz Theorem states is equivalent to solving the standard eigenvalue problem:

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

Essentially, we want to find the __second-smallest eigenvalue__ associated with the matrix $$L = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, often called the (normalized) *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. We'll calculate the eigenvector corresponding to the second-smallest eigenvalue below, and check how our labels perform:


```python
# constructing the Laplacian matrix
L = np.linalg.inv(D)@(D-A)

# find and sort eigenvectors and eigenvalues of L
Lam, U = np.linalg.eig(L)
ix = Lam.argsort()
Lam, U = Lam[ix], U[:,ix]

# find eigenvector corresponding to second-smallest eigenvalue
z_eig = U[:,1]
```


```python
# create array of colors
colors = np.zeros(z_min.size)
colors[z_eig < 0] = 1

# plot data w/ cluster colors
plt.scatter(X[:,0], X[:,1], c = colors)
```

![eig_labeled_moons](/images/eig_labeled_moons.png)

This is much better! Looks like there's only one mislabeled point, so our labels did a pretty great job this time around.

## Part G - Synthesizing our Results

Next, we're going to wrap up all of this discussion on spectral clustering by creating a function `spectral_clustering(X, epsilon)` that will perform the whole clustering process in one function call. Notice that the function isn't excessively long, as we've already written a lot of the infrastructure the function needs to operate. We'll test out our function below as well:


```python
def spectral_clustering(X, epsilon):
    """
    Function that performs spectral clustering on a collection of data points.
    
    X: a matrix containing data points to be labeled
    epsilon: the minimum distance used in calculated the similarity matrix
    return: a one-dimensional array of length n containing cluster labels (ones and zeros)
    """
    
    # create similarity matrix A
    A = similarity_matrix(X, epsilon)
    
    # create diagonal matrix D w/ degree entries
    D = np.zeros((n, n))
    np.fill_diagonal(D, np.sum(A, axis = 0))
    
    # find and sort eigenvectors and eigenvalues of Laplacian matrix
    Lam, U = np.linalg.eig(np.linalg.inv(D)@(D-A))
    U = U[:,Lam.argsort()]

    # return eigenvector corresponding to second-smallest eigenvalue
    return U[:,1]
```


```python
labels = spectral_clustering(X, epsilon)

# create array of colors
colors = np.zeros(labels.size)
colors[labels < 0] = 1

# plot data w/ cluster colors
plt.scatter(X[:,0], X[:,1], c = colors)
```

![spectral_labeled_moons](/images/spectral_labeled_moons.png)

## Part H - Experiments I

Now that we've got the entire spectral clustering process wrapped up nicely into one function, let's test it out on some interesting data!


```python
n = 1000
X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.05, random_state=None)

labels = spectral_clustering(X, epsilon)

# create array of colors
colors = np.zeros(labels.size)
colors[labels < 0] = 1

# plot data w/ cluster colors
#plt.scatter(X[:,0], X[:,1], c = colors)
```

![1000moons](/images/1000moons.png)

Even with $$1000$$ points, it looks like our `spectral_clustering` function did an almost perfect job at labeling the data! Let's take a look at something a bit messier by changing the `noise` argument of the `make_moons` method:


```python
n = 1000
X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.2, random_state=None)

labels = spectral_clustering(X, epsilon)

# create array of colors
colors = np.zeros(labels.size)
colors[labels < 0] = 1

# plot data w/ cluster colors
plt.scatter(X[:,0], X[:,1], c = colors)
```

![noisymoons](/images/noisymoons.png)

Interesting. The above labels seem to do a decent job at distinguishing the two (_admittedly difficult to see_) moons, with a decent amount of overlap.

## Part I - Experiments II

Now, let's try `spectral_clustering` on another data set -- the bull's eye! 


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```

![bullseye](/images/bullseye.png)

First, let's see how `KMeans` performs here:


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![kmeans_bullseye](/images/kmeans_bullseye.png)

Hmmm... Not so good. Let's try `spectral_clustering`, with differing values of `epsilon`.


```python
# epsilon = 1

labels = spectral_clustering(X, 1)

# create array of colors
colors = np.zeros(labels.size)
colors[labels < 0] = 1

# plot data w/ cluster colors
plt.scatter(X[:,0], X[:,1], c = colors)
```

![labeled_bullseye](/images/bullseye1.png)

Looks like that didn't work either. Let's half the value of epsilon now:

```python
# epsilon = 0.5

labels = spectral_clustering(X, 0.5)

# create array of colors
colors = np.zeros(labels.size)
colors[labels < 0] = 1

# plot data w/ cluster colors
plt.scatter(X[:,0], X[:,1], c = colors)
```

![labeled_bullseye](/images/bullseye2.png)

Bullseye!
